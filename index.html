
<html>
<head>
  <meta charset="utf-8">
  <title>Social Distancing | WINLAB CPI 2021</title>
  <link rel="stylesheet" href="/Users/mayashankar/winlabsd2021/static/css/index.css">
</head>
<body>
    <script src="/winlabsd2021/static/js/script.js"></script>
    <div id="intro">
      <h1>Analyzing Social Distancing with Sensory Input</h1>
      <h3>Rutgers WINLAB</h3>
    </div>
    <div id="about">
      <h2>Objective</h2>
      <p>Our core objective is to gauge the effectiveness of social distancing policies on containing the transmission of the coronavirus by investigating social distancing among people in New York City during the 2020 COVID-19 lockdown. We used the data collected on pedestrian activities from the streets of NYC during and after the COVID-19 outbreak from public traffic cameras and vehicle dashcam footage. We used machine learning models YOLOv3 and Super Resolution to pre-process the image data and then created a distance estimation program to study social distancing in different NYC boroughs from March through August 2020.</p>
    </div>
    <div id="pipeline">
      <div id="pipeline-raw">
        <h2>Image Processing</h2>
        <img src="/winlabsd2021/static/imgs/raw%20image.jpg" alt="raw image">
        <p>Our dataset draws on Nexarâ€™s dashcam image directory. This directory is composed of pictures taken from Uber and Lyft vehicles driving through 3 boroughs of New York City -- Manhattan, Queens, and Brooklyn. We primarily focused on images taken in Manhattan during the first 5 months of the pandemic (March through August 2020).</p>

        </div>
        <div id="pipeline-dim" class="many-pics">
          <figure>
            <img src="/winlabsd2021/static/imgs/0_frame_6a1d48409c24d6305f9b8d25100451c0.jpg" alt="dim image">
            <figcaption>Dim image, brightness-per-pixel of 18.3</figcaption>
          </figure>
          <figure>
            <img src="/winlabsd2021/static/imgs/promenade-sur-pamir-highway.jpg" alt="bright image">
            <figcaption>Bright image, brightness-per-pixel of 137.6</figcaption>
          </figure>

          <p>We noticed that the object detection model was havinng trouble identifying people in dim images. A lot of the dashcam images in our dataset were taken at night, making them blurry and unusable. We determined the average brightness-per-pixel of an image and set a threshold of 50 bpp to filter out usable images. <br>This gave us a smaller and more informative set of pictures to work with. </p>

          </div>
          <div id="pipeline-yolo">
            <img src="/winlabsd2021/static/imgs/boxed%20image.jpg" alt="YOLO">
            <p>YOLO (You Only Look Once) is a notable real-time object detection model that classifies objects in a single sweep of the image. We used YOLOv3, trained using the COCO (Common Objects in Context) dataset. OpenCV has a deep neural network (DNN) module that was used to read in YOLOv3 weights.<br><br>
              We paired YOLOv3 with a script to draw colorful boxes around any human YOLO detected. For example, the person walking on the right side of this image has a blue box drawn around them.We sorted out any images where YOLO detected no people, including images with only blurry or partial representations of people. </p>

            </div>
            <div id="pipeline-patch" class="many-pics">
              <figure>
                <img src="/winlabsd2021/static/imgs/box%20EDSR.jpg" alt="EDSR">
                <figcaption>EDSR</figcaption>
              </figure>
              <figure>
                <img src="/winlabsd2021/static/imgs/box%20ESPCN.jpg" alt="ESPCN">
                <figcaption>ESPCN</figcaption>
              </figure>
              <figure>
                <img src="/winlabsd2021/static/imgs/box%20FSRCNN.jpg" alt="FSRCNN">
                <figcaption>FSRCNN</figcaption>
              </figure>
              <figure>
                <img src="/winlabsd2021/static/imgs/box%20LapSRN.jpg" alt="LapSRN">
                <figcaption>LapSRN</figcaption>
              </figure>
              <p>Using the coordinates of a person, we cropped the image around the detected people, creating an image patch. Here, a challenge we faced with the images was picture quality. Traffic and dashcam images are generally low resolution. <br><br>To handle this, we ran the images through a Super Resolution (SR) model. We tested four different models, shown above. Between these four super-resolution methods, ESPCN worked the best, because YOLO detected the most people in images that used ESPCN.</p>
              </div>
              <div id="pipeline-final">
                <img src="/winlabsd2021/static/imgs/image.png" alt="raw image">
                <p>Finally, we ran the filtered set of images through a distance detection model. This model created boxes around humans, this time changing the color based on proximity to the camera. This color ranges from yellow (no people nearby) to dark red (many people in proximity).
<br><br>
For each person, we monitored how many other people were within six feet of them. In this image, the leftmost number above a person is their individual ID, the middle number is the confidence (how sure is YOLO that this is a human?), and the rightmost number is how many people are close to them.</p>

              </div>
            </div>
            <!--
<div id="results">
              <h2>Results</h2>
              <p>Some pictures of graphy things</p>
            </div>
-->
            <div id="people">
              <h2>Who We Are</h2>
              <p style="font-weight: 400;">Advisors: <br>
  Professor Jorge Ortiz, Rutgers University<br>
Tahiya Chowdhury, Rutgers University<br>
  <br>
  Team: <br>
  Justin Ding, Rutgers University<br>
  Taqiya Ehsan, Rutgers University<br>
  Maya Shankar, University of Toronto<br>
  Aryan Doshi, Rutgers University<br>
  Ansh Bhatti, Middlesex County Academy <br>
  Sonia Batheja, Hunterdon Central Regional HS<br></p>
              </div>


</body>
</html>
